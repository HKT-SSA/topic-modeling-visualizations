{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import boto3\n",
    "import wordcloud as wc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide details of the S3 bucket you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='<fill-in-your-bucket-name>'\n",
    "prefix='data/translated/politics-2000'\n",
    "\n",
    "!aws s3api head-bucket --bucket $bucket # verify bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tmp folder for storing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER='output'\n",
    "\n",
    "def make_tmp_folder(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except OSError as e:\n",
    "        print(\"{} folder already exists\".format(folder_name))\n",
    "\n",
    "make_tmp_folder(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the translated text into your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://large-text-understanding/data/THUCNews/translated/political-news-2000/ s3://$bucket/$prefix/ --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Amazon Comprehend to run a topic modeling job\n",
    "\n",
    "You can use Amazon Comprehend to examine the content of a collection of documents to determine common themes without providing pre-labeled data. \n",
    "\n",
    "Amazon Comprehend uses a Latent Dirichlet Allocation-based learning model to determine the topics in a set of documents. It examines each document to determine the context and meaning of a word. The set of words that frequently belong to the same context across the entire document set make up a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s3_location = f's3://{bucket}/{prefix}/'\n",
    "print(f'Topic modeling job input s3 location: {input_s3_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **Amazon Comprehend** console to run a topic modeling job: [https://console.aws.amazon.com/comprehend/home](https://console.aws.amazon.com/comprehend/home)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download topic modeling results from Amazon Comprehend\n",
    "\n",
    "When the topic detection job is finished, the service creates an output file in S3. The `S3Uri` field of the job output configuration contains the location of the output file, called `output.tar.gz`. It is a compressed archive that contains the ouput of the topic detection job: \n",
    "* `topic-terms.csv` - a list of topics in the collection. For each topic, the list includes the top 10 terms by topic according to their weight.\n",
    "* `doc-topics.csv` -lists the documents associated with a topic and the proportion of the document that is concerned with the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling_job_id = '1a99e2c4f15301c878faf973dea174b5' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_client = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_detection_response = comprehend_client.describe_topics_detection_job(\n",
    "    JobId=topic_modeling_job_id\n",
    ")\n",
    "output_file = topics_detection_response['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "num_topics = topics_detection_response['TopicsDetectionJobProperties']['NumberOfTopics']\n",
    "\n",
    "print(f'output file: {output_file}')\n",
    "print(f'number of topics: {num_topics}')\n",
    "\n",
    "download_output=os.path.join(OUTPUT_FOLDER, f'output-{num_topics}-topics.tar.gz')\n",
    "!aws s3 cp $output_file $download_output\n",
    "print(f'downloaded topic modeling output to: {download_output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf $download_output -C $OUTPUT_FOLDER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms_csv = os.path.join(OUTPUT_FOLDER, 'topic-terms.csv')\n",
    "doc_topics_csv = os.path.join(OUTPUT_FOLDER, 'doc-topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the `topic-terms.csv` file content to see the top terms that appear for each topic detected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $topic_terms_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review first few lines of the `doc-topics.csv` file content to see what the output format looks like on which topics are detected for each document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 15 $doc_topics_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial visualizations of the topic modeling result\n",
    "\n",
    "Using `matplotlib` and `wordcloud` we can perform some initial exploration and visualization on the topic modeling output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic visualization using wordcloud\n",
    "`wordcloud` use the size of words to reflect the relative weights of the terms. This can help in having a quick sense of what each topic is about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topic_terms(topic_terms_csv):\n",
    "    topics=defaultdict(dict)\n",
    "    with open(topic_terms_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                print(f'Column names are {\", \".join(row)}')\n",
    "                line_count += 1\n",
    "            else:\n",
    "                topic = row[0]\n",
    "                term = row[1]\n",
    "                freq= float(row[2])\n",
    "                topics[topic][term]=freq\n",
    "                line_count += 1\n",
    "        print(f'Processed {line_count} lines.')\n",
    "    return topics\n",
    "\n",
    "def plot_topic_word_cloud(topics):      \n",
    "    plt.figure(figsize=(20,16))\n",
    "\n",
    "    n_col = 6\n",
    "\n",
    "    for i, item in enumerate(topics):\n",
    "\n",
    "        title_str = 'Topic{}'.format(item)\n",
    "\n",
    "        wordcloud = wc.WordCloud(background_color='white').fit_words(topics[item])\n",
    "\n",
    "        plt.subplot(len(topics) // n_col+1, n_col, i+1)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title_str)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = parse_topic_terms(topic_terms_csv)\n",
    "plot_topic_word_cloud(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic distribution\n",
    "What's the makeup of topics across the whole corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_doc_topic_distribution(doc_topics_csv):\n",
    "    doc_topic_df = pd.read_csv(doc_topics_csv)\n",
    "    return doc_topic_df\n",
    "\n",
    "def summarize_topic_distributions(doc_topic_df):\n",
    "    df_summary = doc_topic_df.groupby(['topic']).sum()\n",
    "    df_summary['proportion'] = df_summary['proportion']/(df_summary['proportion'].sum())\n",
    "    df_summary = df_summary.sort_values(by=['proportion'], ascending=False)\n",
    "    return df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc-topics.csv` output gets loaded into a pandas dataframe. Then we sort the topics by the proprotion they make up of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = parse_doc_topic_distribution(doc_topics_csv)\n",
    "df_summary = summarize_topic_distributions(doc_topic_df)\n",
    "df_summary.plot(kind='bar', figsize=(16,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 topics in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for documents most highly related to given topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_of_interest=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_document_for_topic(topic_of_interest, max_results=10):\n",
    "    filtered_by_topic_df = doc_topic_df[doc_topic_df['topic']==topic_of_interest]\n",
    "    filtered_by_topic_df = filtered_by_topic_df.sort_values(by=['proportion'], ascending=False)\n",
    "    return filtered_by_topic_df.head(max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_docs = find_top_document_for_topic(topic_of_interest, max_results=10)\n",
    "top_10_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = top_10_docs.iloc[0,0]\n",
    "doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/$prefix/$doc_id $OUTPUT_FOLDER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $OUTPUT_FOLDER/$doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results into Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./neptune/neptune_csv_converter/loader.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_vertex_csv = os.path.join(OUTPUT_FOLDER, 'neptune-nodes.csv')\n",
    "neptune_edge_csv = os.path.join(OUTPUT_FOLDER, 'neptune-edges.csv')\n",
    "neptune_vertex_csv_s3_path = f's3://{bucket}/neptune/neptune-nodes.csv'\n",
    "neptune_edge_csv_s3_path = f's3://{bucket}/neptune/neptune-edges.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./neptune/neptune_csv_converter/loader.py --topictermscsv $topic_terms_csv --doctopiccsv $doc_topics_csv --edgeoutput $neptune_edge_csv --vertexoutput $neptune_vertex_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $neptune_vertex_csv $neptune_vertex_csv_s3_path\n",
    "!aws s3 cp $neptune_edge_csv $neptune_edge_csv_s3_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: get output from cloudformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPTUNE_CLUSTER_ENDPOINT='neptunedbcluster-1mylwjtoaeqq.cluster-c8g1tbg0xvzr.us-west-2.neptune.amazonaws.com'\n",
    "NEPTUNE_CLUSTER_PORT=8182\n",
    "NEPTUNE_LOAD_FROM_S3_ROLE_ARN='arn:aws:iam::735324722473:role/topic-modeling-resources-Nep-NeptuneLoadFromS3Role-11WJCVUVVZIF2'\n",
    "AWS_REGION='us-west-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NEPTUNE_CLUSTER_ENDPOINT=$NEPTUNE_CLUSTER_ENDPOINT\n",
    "%env NEPTUNE_CLUSTER_PORT=$NEPTUNE_CLUSTER_PORT\n",
    "%env NEPTUNE_LOAD_FROM_S3_ROLE_ARN=$NEPTUNE_LOAD_FROM_S3_ROLE_ARN\n",
    "%env AWS_REGION=$AWS_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run './neptune/neptune.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.clear(batch_size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_load_params = {\n",
    "    \"source\" : neptune_vertex_csv_s3_path,\n",
    "      \"format\" : \"csv\",\n",
    "      \"iamRoleArn\" : NEPTUNE_LOAD_FROM_S3_ROLE_ARN, \n",
    "      \"region\" : AWS_REGION,  \n",
    "      \"failOnError\" : \"FALSE\", \n",
    "      \"parallelism\" : \"HIGH\" \n",
    "    }\n",
    "\n",
    "vertex_params_json = os.path.join(OUTPUT_FOLDER, 'vertex_params.json')\n",
    "with open(vertex_params_json, 'w') as outfile:\n",
    "    json.dump(vertex_load_params, outfile)\n",
    "    \n",
    "!curl -X POST -H 'Content-Type: application/json' \\\n",
    "    https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader -d @$vertex_params_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_id = \"72eeeb5b-c02a-4415-a80a-0b769de1ef03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -G https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader/$load_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_load_params = {\n",
    "    \"source\" : neptune_edge_csv_s3_path,\n",
    "      \"format\" : \"csv\",\n",
    "      \"iamRoleArn\" : NEPTUNE_LOAD_FROM_S3_ROLE_ARN, \n",
    "      \"region\" : AWS_REGION,  \n",
    "      \"failOnError\" : \"FALSE\", \n",
    "      \"parallelism\" : \"HIGH\" \n",
    "    }\n",
    "\n",
    "edge_params_json = os.path.join(OUTPUT_FOLDER, 'edge_params.json')\n",
    "with open(edge_params_json, 'w') as outfile:\n",
    "    json.dump(edge_load_params, outfile)\n",
    "    \n",
    "!curl -X POST -H 'Content-Type: application/json' \\\n",
    "    https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader -d @$edge_params_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_id = \"a5a1f546-6297-4f8d-a8d3-86f8f15c29fd\"\n",
    "!curl -G https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader/$load_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = neptune.graphTraversal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = g.V().groupCount().by(T.label).toList()\n",
    "edges  = g.E().groupCount().by(T.label).toList()\n",
    "print(vertices)\n",
    "print(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.V().has('topic','name', '001').outE().toList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g.V().hasLabel('topic').toList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nodes = g.V().hasLabel('topic') \\\n",
    "    .project('name', 'id', 'type') \\\n",
    "                .by('name') \\\n",
    "                .by(T.id) \\\n",
    "                .by(T.label) \\\n",
    "                .toList()\n",
    "topic_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_nodes = g.V().hasLabel('term') \\\n",
    "    .project('name', 'id', 'type') \\\n",
    "                .by('name') \\\n",
    "                .by(T.id) \\\n",
    "                .by(T.label) \\\n",
    "                .toList()\n",
    "term_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = g.E().hasLabel('topic_term_weight') \\\n",
    "            .project('source', 'target', 'value') \\\n",
    "            .by(__.outV().id()) \\\n",
    "            .by(__.inV().id()) \\\n",
    "            .by('weight') \\\n",
    "            .toList()\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
