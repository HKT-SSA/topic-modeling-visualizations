{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import boto3\n",
    "import wordcloud as wc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide details of the S3 bucket you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='<fill-in-your-bucket-name>'\n",
    "prefix='data/translated/politics-2000'\n",
    "\n",
    "!aws s3api head-bucket --bucket $bucket # verify bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tmp folder for storing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER='output'\n",
    "\n",
    "def make_tmp_folder(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except OSError as e:\n",
    "        print(\"{} folder already exists\".format(folder_name))\n",
    "\n",
    "make_tmp_folder(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the translated text into your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://large-text-understanding/data/THUCNews/translated/political-news-2000/ s3://$bucket/$prefix/ --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Amazon Comprehend to run a topic modeling job\n",
    "\n",
    "You can use Amazon Comprehend to examine the content of a collection of documents to determine common themes without providing pre-labeled data. \n",
    "\n",
    "Amazon Comprehend uses a Latent Dirichlet Allocation-based learning model to determine the topics in a set of documents. It examines each document to determine the context and meaning of a word. The set of words that frequently belong to the same context across the entire document set make up a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s3_location = f's3://{bucket}/{prefix}/'\n",
    "print(f'Topic modeling job input s3 location: {input_s3_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **Amazon Comprehend** console to run a topic modeling job: [https://console.aws.amazon.com/comprehend/home](https://console.aws.amazon.com/comprehend/home)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download topic modeling results from Amazon Comprehend\n",
    "\n",
    "When the topic detection job is finished, the service creates an output file in S3. The `S3Uri` field of the job output configuration contains the location of the output file, called `output.tar.gz`. It is a compressed archive that contains the ouput of the topic detection job: \n",
    "* `topic-terms.csv` - a list of topics in the collection. For each topic, the list includes the top 10 terms by topic according to their weight.\n",
    "* `doc-topics.csv` -lists the documents associated with a topic and the proportion of the document that is concerned with the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling_job_id = '<fill-in-job-id>' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_client = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_detection_response = comprehend_client.describe_topics_detection_job(\n",
    "    JobId=topic_modeling_job_id\n",
    ")\n",
    "output_file = topics_detection_response['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "num_topics = topics_detection_response['TopicsDetectionJobProperties']['NumberOfTopics']\n",
    "\n",
    "print(f'output file: {output_file}')\n",
    "print(f'number of topics: {num_topics}')\n",
    "\n",
    "download_output=os.path.join(OUTPUT_FOLDER, f'output-{num_topics}-topics.tar.gz')\n",
    "!aws s3 cp $output_file $download_output\n",
    "print(f'downloaded topic modeling output to: {download_output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf $download_output -C $OUTPUT_FOLDER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms_csv = os.path.join(OUTPUT_FOLDER, 'topic-terms.csv')\n",
    "doc_topics_csv = os.path.join(OUTPUT_FOLDER, 'doc-topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the `topic-terms.csv` file content to see the top terms that appear for each topic detected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $topic_terms_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review first few lines of the `doc-topics.csv` file content to see what the output format looks like on which topics are detected for each document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 15 $doc_topics_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial visualizations of the topic modeling result\n",
    "\n",
    "Using `matplotlib` and `wordcloud` we can perform some initial exploration and visualization on the topic modeling output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic visualization using wordcloud\n",
    "`wordcloud` use the size of words to reflect the relative weights of the terms. This can help in having a quick sense of what each topic is about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topic_terms(topic_terms_csv):\n",
    "    topics=defaultdict(dict)\n",
    "    with open(topic_terms_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                print(f'Column names are {\", \".join(row)}')\n",
    "                line_count += 1\n",
    "            else:\n",
    "                topic = row[0]\n",
    "                term = row[1]\n",
    "                freq= float(row[2])\n",
    "                topics[topic][term]=freq\n",
    "                line_count += 1\n",
    "        print(f'Processed {line_count} lines.')\n",
    "    return topics\n",
    "\n",
    "def plot_topic_word_cloud(topics):      \n",
    "    plt.figure(figsize=(20,16))\n",
    "\n",
    "    n_col = 6\n",
    "\n",
    "    for i, item in enumerate(topics):\n",
    "\n",
    "        title_str = 'Topic{}'.format(item)\n",
    "\n",
    "        wordcloud = wc.WordCloud(background_color='white').fit_words(topics[item])\n",
    "\n",
    "        plt.subplot(len(topics) // n_col+1, n_col, i+1)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title_str)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = parse_topic_terms(topic_terms_csv)\n",
    "plot_topic_word_cloud(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic distribution\n",
    "What's the makeup of topics across the whole corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_doc_topic_distribution(doc_topics_csv):\n",
    "    doc_topic_df = pd.read_csv(doc_topics_csv)\n",
    "    return doc_topic_df\n",
    "\n",
    "def summarize_topic_distributions(doc_topic_df):\n",
    "    df_summary = doc_topic_df.groupby(['topic']).sum()\n",
    "    df_summary['proportion'] = df_summary['proportion']/(df_summary['proportion'].sum())\n",
    "    df_summary = df_summary.sort_values(by=['proportion'], ascending=False)\n",
    "    return df_summary\n",
    "\n",
    "def summarize_topic_frequency(doc_topic_df):\n",
    "    df_summary = doc_topic_df.groupby('topic').size().to_frame()\n",
    "    df_summary.columns = ['count']\n",
    "    df_summary = df_summary.sort_values(by=['count'], ascending=False)\n",
    "    return df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc-topics.csv` output gets loaded into a pandas dataframe. Then we sort the topics by the proprotion they make up of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = parse_doc_topic_distribution(doc_topics_csv)\n",
    "topic_distribution_df = summarize_topic_distributions(doc_topic_df)\n",
    "topic_distribution_df.plot(kind='bar', figsize=(16,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_frequency_df =  summarize_topic_frequency(doc_topic_df)\n",
    "topic_frequency_df.plot(kind='bar', figsize=(16,4),  title='topic distribution by document count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 5 topics in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 5 topics in the corpus by number of documents containing it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_frequency_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for documents most highly related to given topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_of_interest=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_document_for_topic(topic_of_interest, max_results=10):\n",
    "    filtered_by_topic_df = doc_topic_df[doc_topic_df['topic']==topic_of_interest]\n",
    "    filtered_by_topic_df = filtered_by_topic_df.sort_values(by=['proportion'], ascending=False)\n",
    "    return filtered_by_topic_df.head(max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_docs = find_top_document_for_topic(topic_of_interest, max_results=10)\n",
    "top_10_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = top_10_docs.iloc[0,0]\n",
    "doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/$prefix/$doc_id $OUTPUT_FOLDER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $OUTPUT_FOLDER/$doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a graph database (Amazon Neptune) to better explore the topic modeling output\n",
    "\n",
    "Using a graph database such as [Amazon Neptune](https://aws.amazon.com/neptune/) allows us to understand the relationships between topics and documents in a more natural way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results into Amazon Neptune\n",
    "\n",
    "To [load data into Neptune](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html), we will be using the Neptune **Load** command. The **Load** API requires the input to be in a [specific CSV format](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format-gremlin.html). Therefore, we will first convert our topic modeling output files into the required format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert result files to Neptune accepted format\n",
    "We need to convert our data into 2 csv files:\n",
    "* vertex csv file  (represents nodes in the graph) \n",
    "* edge  csv file (represents edges in the graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_vertex_csv = os.path.join(OUTPUT_FOLDER, 'neptune-nodes.csv')\n",
    "neptune_edge_csv = os.path.join(OUTPUT_FOLDER, 'neptune-edges.csv')\n",
    "neptune_vertex_csv_s3_path = f's3://{bucket}/neptune/neptune-nodes.csv'\n",
    "neptune_edge_csv_s3_path = f's3://{bucket}/neptune/neptune-edges.csv'\n",
    "\n",
    "!python ./neptune/neptune_csv_converter/loader.py --topictermscsv $topic_terms_csv --doctopiccsv $doc_topics_csv --edgeoutput $neptune_edge_csv --vertexoutput $neptune_vertex_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $neptune_vertex_csv $neptune_vertex_csv_s3_path\n",
    "!aws s3 cp $neptune_edge_csv $neptune_edge_csv_s3_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Neptune endpoint parameters \n",
    "Get parameters such as the neptune endpoint from cloudformation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudformation_stack_name='large-text-understanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('cloudformation')\n",
    "\n",
    "response = client.describe_stacks(\n",
    "    StackName=cloudformation_stack_name,\n",
    "    NextToken='string'\n",
    ")\n",
    "\n",
    "cloudformation_outputs=response['Stacks'][0]['Outputs']\n",
    "\n",
    "NEPTUNE_CLUSTER_ENDPOINT=''\n",
    "NEPTUNE_LOAD_FROM_S3_ROLE_ARN=''\n",
    "NEPTUNE_CLUSTER_PORT=8182\n",
    "AWS_REGION=''\n",
    "for output in cloudformation_outputs: \n",
    "    if output['OutputKey'] == 'NeptuneEndpoint':\n",
    "        NEPTUNE_CLUSTER_ENDPOINT = output['OutputValue'] \n",
    "    if output['OutputKey'] == 'NeptuneLoadFromS3IAMRoleArn':\n",
    "        NEPTUNE_LOAD_FROM_S3_ROLE_ARN = output['OutputValue'] \n",
    "    if output['OutputKey'] == 'AWSRegion':\n",
    "        AWS_REGION = output['OutputValue'] \n",
    "\n",
    "\n",
    "%env NEPTUNE_CLUSTER_ENDPOINT=$NEPTUNE_CLUSTER_ENDPOINT\n",
    "%env NEPTUNE_CLUSTER_PORT=$NEPTUNE_CLUSTER_PORT\n",
    "%env NEPTUNE_LOAD_FROM_S3_ROLE_ARN=$NEPTUNE_LOAD_FROM_S3_ROLE_ARN\n",
    "%env AWS_REGION=$AWS_REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run './neptune/neptune.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean existing data in the Neptune database (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.clear(batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the nodes csv into Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_load_params = {\n",
    "    \"source\" : neptune_vertex_csv_s3_path,\n",
    "      \"format\" : \"csv\",\n",
    "      \"iamRoleArn\" : NEPTUNE_LOAD_FROM_S3_ROLE_ARN, \n",
    "      \"region\" : AWS_REGION,  \n",
    "      \"failOnError\" : \"FALSE\", \n",
    "      \"parallelism\" : \"HIGH\" \n",
    "    }\n",
    "\n",
    "vertex_params_json = os.path.join(OUTPUT_FOLDER, 'vertex_params.json')\n",
    "with open(vertex_params_json, 'w') as outfile:\n",
    "    json.dump(vertex_load_params, outfile)\n",
    "    \n",
    "!curl -X POST -H 'Content-Type: application/json' \\\n",
    "    https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader -d @$vertex_params_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace below with load id from the above response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_id = \"<replace-with-loadId>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -G https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader/$load_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!! ONLY PROCEED IF THE ABOVE SHOWS A STATUS OF `LOAD_COMPLETED`\n",
    "\n",
    "Once all the nodes are loaded to the database. Now we load all the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_load_params = {\n",
    "    \"source\" : neptune_edge_csv_s3_path,\n",
    "      \"format\" : \"csv\",\n",
    "      \"iamRoleArn\" : NEPTUNE_LOAD_FROM_S3_ROLE_ARN, \n",
    "      \"region\" : AWS_REGION,  \n",
    "      \"failOnError\" : \"FALSE\", \n",
    "      \"parallelism\" : \"HIGH\" \n",
    "    }\n",
    "\n",
    "edge_params_json = os.path.join(OUTPUT_FOLDER, 'edge_params.json')\n",
    "with open(edge_params_json, 'w') as outfile:\n",
    "    json.dump(edge_load_params, outfile)\n",
    "    \n",
    "!curl -X POST -H 'Content-Type: application/json' \\\n",
    "    https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader -d @$edge_params_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace below with load id from the above response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_id = \"<replace-with-loadId>\"\n",
    "!curl -G https://$NEPTUNE_CLUSTER_ENDPOINT:$NEPTUNE_CLUSTER_PORT/loader/$load_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the graph using Gremlin\n",
    "\n",
    "Amazon Neptune is compatible with [Apache TinkerPop3](https://tinkerpop.apache.org/docs/current/reference/#intro) and Gremlin 3.4.1. This means that you can connect to a Neptune DB instance and use the Gremlin traversal language to query the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use Gremlin to verify we have the right number of nodes and edges loaded into our database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = neptune.graphTraversal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = g.V().groupCount().by(T.label).toList()\n",
    "edges  = g.E().groupCount().by(T.label).toList()\n",
    "print('nodes:')\n",
    "print(vertices)\n",
    "print('edges:')\n",
    "print(edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a web app to interactively explore graph data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an development environment:\n",
    "1. if you have `npm`, `node`, `git` installed on your laptop, you can choose to use your laptop\n",
    "1. alternatively, open [Cloud9](https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2) and create a new cloud9 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On either your laptop or the cloud9 environment:\n",
    "1. In a terminal window: clone the github repo \n",
    "    ```\n",
    "    git clone https://github.com/angelarw/topic-modeling-visualizations\n",
    "    ```\n",
    "1. open the `topic-modeling-visualizations/webapp/src/amplify-config.js` file. Replace all configuration with values from the Cloudformation output\n",
    "1. Install angular cli\n",
    "   ```\n",
    "   npm install -g @angular/cli\n",
    "   ```\n",
    "1. Install code dependencies \n",
    "   ```\n",
    "   cd topic-modeling-visualizations/webapp/\n",
    "   npm install\n",
    "   ```\n",
    "1. Run the webapp \n",
    "   ```\n",
    "   ng serve --port 8080 --disable-host-check\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
